{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supermarket Sales Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:21.753178Z",
     "iopub.status.busy": "2020-10-02T14:07:21.752081Z",
     "iopub.status.idle": "2020-10-02T14:07:23.049135Z",
     "shell.execute_reply": "2020-10-02T14:07:23.048271Z"
    },
    "id": "BDaztn-0sSNK",
    "outputId": "4260e705-1e31-4676-af35-76c4fa2ea614",
    "papermill": {
     "duration": 1.35302,
     "end_time": "2020-10-02T14:07:23.049274",
     "exception": false,
     "start_time": "2020-10-02T14:07:21.696254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:23.167957Z",
     "iopub.status.busy": "2020-10-02T14:07:23.167172Z",
     "iopub.status.idle": "2020-10-02T14:07:23.201310Z",
     "shell.execute_reply": "2020-10-02T14:07:23.200290Z"
    },
    "id": "87G1rk0bse0U",
    "papermill": {
     "duration": 0.099226,
     "end_time": "2020-10-02T14:07:23.201472",
     "exception": false,
     "start_time": "2020-10-02T14:07:23.102246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('supermarket_sales - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:23.712363Z",
     "iopub.status.busy": "2020-10-02T14:07:23.704465Z",
     "iopub.status.idle": "2020-10-02T14:07:23.724196Z",
     "shell.execute_reply": "2020-10-02T14:07:23.726021Z"
    },
    "id": "xjdbJRmgs97V",
    "outputId": "c5a0cc34-7b69-49f8-994a-8fbe6292d7e8",
    "papermill": {
     "duration": 0.089359,
     "end_time": "2020-10-02T14:07:23.726276",
     "exception": false,
     "start_time": "2020-10-02T14:07:23.636917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:23.988423Z",
     "iopub.status.busy": "2020-10-02T14:07:23.987138Z",
     "iopub.status.idle": "2020-10-02T14:07:23.994756Z",
     "shell.execute_reply": "2020-10-02T14:07:23.993612Z"
    },
    "id": "x_L3arKAs_mn",
    "papermill": {
     "duration": 0.087568,
     "end_time": "2020-10-02T14:07:23.995010",
     "exception": false,
     "start_time": "2020-10-02T14:07:23.907442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Time\", axis=1, inplace=True)\n",
    "df.set_index('Date',inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:24.848031Z",
     "iopub.status.busy": "2020-10-02T14:07:24.847235Z",
     "iopub.status.idle": "2020-10-02T14:07:24.885218Z",
     "shell.execute_reply": "2020-10-02T14:07:24.885784Z"
    },
    "id": "atRsSXGkwlp2",
    "outputId": "bda0727d-63dc-4d9c-a669-6cd5a149c851",
    "papermill": {
     "duration": 0.10506,
     "end_time": "2020-10-02T14:07:24.885982",
     "exception": false,
     "start_time": "2020-10-02T14:07:24.780922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:25.007958Z",
     "iopub.status.busy": "2020-10-02T14:07:25.007027Z",
     "iopub.status.idle": "2020-10-02T14:07:25.011400Z",
     "shell.execute_reply": "2020-10-02T14:07:25.010737Z"
    },
    "id": "TIeDaeDS1kzk",
    "outputId": "53c1aab8-3009-4011-98ad-42c1f6209f33",
    "papermill": {
     "duration": 0.069763,
     "end_time": "2020-10-02T14:07:25.011533",
     "exception": false,
     "start_time": "2020-10-02T14:07:24.941770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dgHJp69bTj6p",
    "papermill": {
     "duration": 0.061163,
     "end_time": "2020-10-02T14:07:26.464402",
     "exception": false,
     "start_time": "2020-10-02T14:07:26.403239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:26.593543Z",
     "iopub.status.busy": "2020-10-02T14:07:26.592750Z",
     "iopub.status.idle": "2020-10-02T14:07:27.238357Z",
     "shell.execute_reply": "2020-10-02T14:07:27.238998Z"
    },
    "id": "GqaxUnndUFvh",
    "outputId": "eb655162-6b50-43b7-d9e0-e368d3bc6333",
    "papermill": {
     "duration": 0.71382,
     "end_time": "2020-10-02T14:07:27.239194",
     "exception": false,
     "start_time": "2020-10-02T14:07:26.525374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group the data by customer type and calculate the average total amount spent by each type\n",
    "df_customer_type = df.groupby('Customer type')['Total'].mean()\n",
    "print(df_customer_type)\n",
    "\n",
    "# visualize the df_customer_type\n",
    "df_customer_type.plot(kind='bar', title = 'Average Total Amount Spent By Customer type ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by customer type and calculate the average total amount spent by each type\n",
    "df_genderr = df.groupby('Gender')['Total'].mean()\n",
    "print(df_genderr)\n",
    "\n",
    "# visualize the df_customer_type\n",
    "df_genderr.plot(kind='bar', title = 'Average Total Amount Spent By Gender');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a Gender based comparison related to Product Line\n",
    "sns.countplot(y ='Product line', hue = \"Gender\", data = df) \n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Product Line');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a Gender based comparison related to Product Line\n",
    "sns.catplot(x='Product line',y='Unit price',hue='Gender',data=df,aspect=3)\n",
    "plt.xlabel('Product Line')\n",
    "plt.ylabel('Unit Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average unit price for each product line\n",
    "df_product_line_price = df.groupby('Product line')['Unit price'].mean()\n",
    "df_product_line_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the average unit price for each product line\n",
    "df_product_line_price.plot(kind='bar', title = 'Average Unit Price For Each Product Line');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the overall gross margin percentage\n",
    "df['gross_margin'] = (df['Total'] - df['cogs']) / df['Total']\n",
    "overall_gross_margin = df['gross_margin'].mean()\n",
    "\n",
    "print('the overall gross margin is', overall_gross_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total sales for each city\n",
    "df_city_sales = df.groupby('City')['Total'].sum()\n",
    "\n",
    "# create a bar chart to visualize the total sales for each city\n",
    "df_city_sales.plot(kind='bar')\n",
    "plt.title('Total Sales by City')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Total Sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a City based comparison related to Product Line\n",
    "sns.catplot(x='Product line',y='Unit price',hue='City',data=df,aspect=3)\n",
    "plt.xlabel('Product Line')\n",
    "plt.ylabel('Unit Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pie chart to visualize the distribution of sales across different product lines\n",
    "df.groupby('Product line')['Total'].sum().plot(kind='bar')\n",
    "plt.title('Sales by Product Line');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot to visualize the relationship between unit price and quantity\n",
    "plt.scatter(df['Unit price'], df['Quantity'])\n",
    "\n",
    "plt.title('Unit Price vs Quantity')\n",
    "plt.xlabel('Unit Price')\n",
    "plt.ylabel('Quantity');\n",
    "\n",
    "df[[\"Unit price\", \"Quantity\"]].corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a correlation value of **0.010778** indicates a very weak positive correlation between the two variables. This means that there is a very weak relationship between the two variables and as one variable increases, the other variable also increases, but only slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:29.388907Z",
     "iopub.status.busy": "2020-10-02T14:07:29.388124Z",
     "iopub.status.idle": "2020-10-02T14:07:29.690811Z",
     "shell.execute_reply": "2020-10-02T14:07:29.691408Z"
    },
    "id": "o-imBfJWxKjS",
    "outputId": "6987bc83-bb9d-4189-9629-7487c036d796",
    "papermill": {
     "duration": 0.39519,
     "end_time": "2020-10-02T14:07:29.691581",
     "exception": false,
     "start_time": "2020-10-02T14:07:29.296391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a histogram to visualize the distribution of customer ratings\n",
    "sns.displot(df['Rating'],kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:30.404457Z",
     "iopub.status.busy": "2020-10-02T14:07:30.403474Z",
     "iopub.status.idle": "2020-10-02T14:07:31.895787Z",
     "shell.execute_reply": "2020-10-02T14:07:31.895105Z"
    },
    "id": "Hal1dPnqylo1",
    "outputId": "e8c20e9e-a9ba-4062-8f2a-2f7e72acff7e",
    "papermill": {
     "duration": 1.569718,
     "end_time": "2020-10-02T14:07:31.895973",
     "exception": false,
     "start_time": "2020-10-02T14:07:30.326255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the gross margin percentage for each branch\n",
    "df.groupby('Branch')['gross margin percentage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:39.321203Z",
     "iopub.status.busy": "2020-10-02T14:07:39.317394Z",
     "iopub.status.idle": "2020-10-02T14:07:39.598392Z",
     "shell.execute_reply": "2020-10-02T14:07:39.597226Z"
    },
    "id": "6D6Hjo9Ua_Ub",
    "outputId": "211e383d-1b04-442d-ba11-4a9202eab01d",
    "papermill": {
     "duration": 0.382748,
     "end_time": "2020-10-02T14:07:39.598535",
     "exception": false,
     "start_time": "2020-10-02T14:07:39.215787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the most used payment method for Product Line\n",
    "sns.countplot(y ='Product line', hue = \"Payment\", data = df) \n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Product Line');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:39.824480Z",
     "iopub.status.busy": "2020-10-02T14:07:39.809768Z",
     "iopub.status.idle": "2020-10-02T14:07:40.028933Z",
     "shell.execute_reply": "2020-10-02T14:07:40.028162Z"
    },
    "papermill": {
     "duration": 0.330347,
     "end_time": "2020-10-02T14:07:40.029069",
     "exception": false,
     "start_time": "2020-10-02T14:07:39.698722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the most used payment method for Branch \n",
    "sns.countplot(y ='Branch', hue = \"Payment\", data = df) \n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Branch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:40.242640Z",
     "iopub.status.busy": "2020-10-02T14:07:40.241366Z",
     "iopub.status.idle": "2020-10-02T14:07:40.465912Z",
     "shell.execute_reply": "2020-10-02T14:07:40.465174Z"
    },
    "papermill": {
     "duration": 0.337103,
     "end_time": "2020-10-02T14:07:40.466037",
     "exception": false,
     "start_time": "2020-10-02T14:07:40.128934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Finding the most used payment method for each City\n",
    "sns.countplot(y ='City', hue = \"Payment\", data = df) \n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Product Line');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.104521,
     "end_time": "2020-10-02T14:07:40.675417",
     "exception": false,
     "start_time": "2020-10-02T14:07:40.570896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Finding Which Branch has better sale for a particular Product Line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:40.891080Z",
     "iopub.status.busy": "2020-10-02T14:07:40.889848Z",
     "iopub.status.idle": "2020-10-02T14:07:41.173584Z",
     "shell.execute_reply": "2020-10-02T14:07:41.172580Z"
    },
    "papermill": {
     "duration": 0.393289,
     "end_time": "2020-10-02T14:07:41.173783",
     "exception": false,
     "start_time": "2020-10-02T14:07:40.780494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding Which Branch has better sale for a particular Product Line\n",
    "sns.countplot(y ='Product line', hue = \"Branch\", data = df) \n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Product Line');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-02T14:07:42.522986Z",
     "iopub.status.busy": "2020-10-02T14:07:42.521967Z",
     "iopub.status.idle": "2020-10-02T14:07:44.310166Z",
     "shell.execute_reply": "2020-10-02T14:07:44.310739Z"
    },
    "id": "JvxBEKr5Ix5A",
    "outputId": "cbff5fbb-8be8-4ea5-f70d-f6425c93f9eb",
    "papermill": {
     "duration": 1.908648,
     "end_time": "2020-10-02T14:07:44.310932",
     "exception": false,
     "start_time": "2020-10-02T14:07:42.402284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "wordcloud = WordCloud(background_color='White').generate(\" \".join(df['Product line']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig('cast.png');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use K-Means clustering and PCA to segment customers based on their purchasing behavior and visualize the results in two-dimensional space. This type of analysis can be useful for businesses to understand the behavior of their customers and develop targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a new DataFrame with only the relevant columns\n",
    "df_clustering = df[['Customer type', 'Gender', 'Total']]\n",
    "\n",
    "# apply one-hot encoding to the categorical columns\n",
    "df_clustering = pd.get_dummies(df_clustering)\n",
    "\n",
    "# reduce the dimensionality of the data using PCA\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_clustering)\n",
    "\n",
    "# initialize the K-Means model with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "\n",
    "# fit the model to the data\n",
    "kmeans.fit(df_pca)\n",
    "\n",
    "# predict the clusters for each data point\n",
    "predicted_clusters = kmeans.predict(df_pca)\n",
    "\n",
    "# add the predicted clusters as a new column to the DataFrame\n",
    "df_clustering['Segment'] = predicted_clusters\n",
    "\n",
    "# group the data by segment and calculate the average total amount spent by each segment\n",
    "df_segment_sales = df_clustering.groupby('Segment')['Total'].mean()\n",
    "print(df_segment_sales)\n",
    "\n",
    "# plot the clusters\n",
    "plt.scatter(df_pca[:,0], df_pca[:,1], c=predicted_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a Random Forest classification algorithm to predict customer ratings based on other attributes in the dataset. By predicting customer ratings, we can gain insights into the factors that influence customer satisfaction and potentially make improvements to our services to increase satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# create a new DataFrame with only the relevant columns\n",
    "df_classification = df[['Customer type', 'Gender', 'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Total', 'Payment', 'cogs', 'gross margin percentage', 'gross income', 'Rating']]\n",
    "\n",
    "# apply one-hot encoding to the categorical columns\n",
    "df_classification = pd.get_dummies(df_classification)\n",
    "\n",
    "# convert the continuous target variable into a categorical variable by binning the values into 3 bins\n",
    "df_classification['Rating'] = pd.cut(df_classification['Rating'], bins=3, labels=[1, 2, 3])\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_classification.drop('Rating', axis=1), df_classification['Rating'], test_size=0.2)\n",
    "\n",
    "# initialize the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict the ratings for the test data\n",
    "predicted_ratings = rf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, predicted_ratings)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# generate a classification report to see precision, recall, f1-score for each class\n",
    "class_report = classification_report(y_test, predicted_ratings)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_test, predicted_ratings)\n",
    "rmse = mean_squared_error(y_test, predicted_ratings, squared=False)\n",
    "mae = mean_absolute_error(y_test, predicted_ratings)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest classification algorithm achieved an accuracy of 0.325, which indicates that the model was not able to accurately predict the customer ratings based on the other attributes in the dataset. Looking at the classification report, we can see that the precision, recall, and f1-score for each class are relatively low, with the highest being 0.41 for precision in class 1. This suggests that the model may not have captured the complex relationships between the attributes and the target variable, and that further data preprocessing and feature engineering may be necessary to improve the model's performance. Additionally, it may be worth considering other classification algorithms or hyperparameter tuning to see if they can achieve better results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report, we can see that the overall accuracy of the model is low at 0.26. The precision, recall, and f1-score for each class are also quite low, with the highest being 0.39 for class 1. This means that the model is not able to accurately predict the customer ratings based on the other attributes in the dataset.\n",
    "\n",
    "To improve the performance of the model, we could try using a different classification algorithm, such as SVM or logistic regression, or we could try optimizing the parameters of the Random Forest algorithm using techniques like grid search or random search. Additionally, we could consider adding more features to the dataset or performing feature engineering to better capture the factors that influence customer ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# create a new DataFrame with only the relevant columns\n",
    "df_classification = df[['Customer type', 'Gender', 'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Total', 'Payment', 'cogs', 'gross margin percentage', 'gross income', 'Rating']]\n",
    "\n",
    "# apply one-hot encoding to the categorical columns\n",
    "df_classification = pd.get_dummies(df_classification)\n",
    "\n",
    "# convert the continuous target variable into a categorical variable by binning the values into 3 bins\n",
    "df_classification['Rating'] = pd.cut(df_classification['Rating'], bins=3, labels=[1, 2, 3])\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_classification.drop('Rating', axis=1), df_classification['Rating'], test_size=0.2)\n",
    "\n",
    "# initialize the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# perform a grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters found\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "# predict the ratings for the test data using the best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "predicted_ratings = best_rf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy of the predictions\n",
    "accuracy = best_rf.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# print a classification report\n",
    "print(classification_report(y_test, predicted_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mse = mean_squared_error(y_test, predicted_ratings)\n",
    "rmse = mean_squared_error(y_test, predicted_ratings, squared=False)\n",
    "mae = mean_absolute_error(y_test, predicted_ratings)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's performance has improved slightly, with an increase in accuracy. However, the precision, recall, and F1-scores for each class are still relatively low, indicating that the model is not performing very well for any individual class. This suggests that more work may be needed to improve the model's performance, such as further feature engineering or trying different algorithms.\n",
    "\n",
    "In this case, the MSE value is relatively low, indicating good performance of the model. The RMSE value is also low, indicating that the model has low variance and is able to make accurate predictions consistently. The MAE value is also relatively low, indicating good performance of the model. Overall, these metrics suggest that the model is performing well in predicting customer ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a new DataFrame with only the relevant columns\n",
    "df_regression = df[['Branch', 'City', 'Customer type', 'Gender', 'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Payment', 'cogs', 'gross margin percentage', 'gross income', 'Total']]\n",
    "\n",
    "# apply one-hot encoding to the categorical columns\n",
    "df_regression = pd.get_dummies(df_regression)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_regression.drop('Total', axis=1), df_regression['Total'], test_size=0.2)\n",
    "\n",
    "# initialize the Random Forest model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# fit the model to the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict the total sales for the test data\n",
    "predicted_totals = rf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy of the predictions\n",
    "accuracy = rf.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value of 0.9999 indicates that the model has made highly precise predictions on the test data, suggesting that it has successfully learned the relationship between the input features and the target variable, Total sales. This implies that the model can be useful in predicting total sales for new data and in improving our marketing and sales strategies to increase revenue.\n",
    "\n",
    "It is essential to bear in mind, though, that while accuracy is a significant indicator of model performance, it is not the sole criterion to consider. Other metrics, such as precision, recall, and F1 score, can also be used to assess the effectiveness of a regression model. These metrics can assist in evaluating the model's performance and making adjustments to increase its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame with only the relevant columns\n",
    "df_new = df[['Branch', 'City', 'Customer type', 'Gender', 'Product line', 'Unit price', 'Quantity', 'Tax 5%', 'Payment', 'cogs', 'gross margin percentage', 'gross income']]\n",
    "\n",
    "# apply one-hot encoding to the categorical columns\n",
    "df_new = pd.get_dummies(df_new)\n",
    "\n",
    "# predict the total sales for the new data\n",
    "predicted_totals = rf.predict(df_new)\n",
    "\n",
    "# print the predicted total sales\n",
    "print(predicted_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the predictions and the index from the original dataset\n",
    "predicted_df = pd.DataFrame(predicted_totals, columns=['predicted_total_sales'])\n",
    "predicted_df.index = df.index\n",
    "\n",
    "# save the dataframe to a CSV file\n",
    "predicted_df.to_csv('predicted_totals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of predicted sales\n",
    "plt.hist(predicted_df['predicted_total_sales'], bins=10)\n",
    "plt.title('Histogram of Predicted Sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a scatter plot of predicted sales vs actual sales\n",
    "plt.scatter(predicted_df['predicted_total_sales'], df['Total'], linewidth=0.1)\n",
    "plt.xlabel('Predicted Sales')\n",
    "plt.ylabel('Actual Sales')\n",
    "plt.title('Predicted vs Actual Sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart of predicted sales by product line\n",
    "plt.bar(df['Product line'], predicted_df['predicted_total_sales'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Product Line')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.title('Predicted Sales by Product Line');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Based on the analysis of the supermarket dataset, the following recommendations can be made:\n",
    "\n",
    "To maximize profits, efforts should be focused on increasing the average unit price and gross margin percentage. Additionally, promoting product lines that have high quantity sold can also boost revenue.\n",
    "\n",
    "Targeting specific cities and customer types can increase the number of transactions. Improving the supermarket rating by gender can also lead to increased customer satisfaction and loyalty.\n",
    "\n",
    "Implementing strategies to minimize the amount of tax paid can improve overall profitability.\n",
    "\n",
    "The Random Forest regression algorithm performed well in predicting total sales for each city and should be used in future sales forecasting. Incorporating other relevant attributes in the dataset can further improve prediction accuracy and aid in making informed business decisions.\n",
    "\n",
    "However, the Random Forest classification algorithm performed relatively poorly in predicting customer ratings. It is recommended to explore alternative algorithms or incorporate additional relevant data to improve prediction accuracy and gain insights into customer preferences for enhancing customer satisfaction and loyalty.\n",
    "\n",
    "Overall, these recommendations can help the supermarket make data-driven decisions and allocate resources effectively, leading to improved profitability and customer satisfaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "papermill": {
   "duration": 29.400034,
   "end_time": "2020-10-02T14:07:44.747361",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-02T14:07:15.347327",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
